import io, json, nltk, pprint, sklearn
from nltk.corpus import stopwords
from nltk.stem.lancaster import LancasterStemmer
from sklearn.naive_bayes import BernoulliNB


word_features = {
	"lawy",
	"bon",
	"kidney",
	"stapl",
	"corp",
	"war",
	"puppy",
	"needy",
	"stretch",
	"subsist",
	"admit",
	"morn",
	"study",
	"dec",
	"shitty",
	"across",
	"involv",
	"fre",
	"ten",
	"sev",
	"til",
	"thes",
	"ent",
	"mus",
	"cousin",
	"spaghett",
	"nyc",
	"fast",
	"funny",
	"sing",
	"tap",
	"sub",
	"crust",
	"explain",
	"city",
	"saus",
	"guit",
	"opportun",
	"microwav",
	"vehic",
	"wallet",
	"medicin",
	"stick",
	"alcohol",
	"nat",
	"frank",
	"outsid",
	"bitch",
	"suff",
	"shel",
	"bed",
	"choos",
	"limit",
	"friend",
	"bro",
	"contact",
	"east",
	"refus",
	"dud",
	"transport",
	"dream",
	"remov",
	"wed",
	"extrem",
	"tast",
	"fir",
	"ala",
	"law",
	"laptop",
	"intern",
	"wal",
	"nc",
	"flu",
	"saf",
	"enco",
	"west",
	"everyday",
	"nurs",
	"ac",
	"zero",
	"injury",
	"grandm",
	"cel",
	"ye",
	"guest",
	"coff",
	"clear",
	"dump",
	"snap",
	"reg",
	"ankl",
	"glory",
	"video",
	"direct",
	"favo",
	"hah",
	"cool",
	"hotel",
	"wing",
	"priv",
	"comply",
	"patch",
	"beer",
	"dead",
	"track",
	"effort",
	"idiot",
	"strong",
	"known",
	"photoshop",
	"ps",
	"freak",
	"tabl",
	"rid",
	"although",
	"result",
	"obvy",
	"bus",
	"stol",
	"country",
	"sod",
	"smel",
	"flo",
	"dark",
	"via",
	"yr",
	"ontario",
	"licens",
	"play",
	"necess",
	"lab",
	"rain",
	"nearby",
	"info",
	"key",
	"terr",
	"uk",
	"ms",
	"cont",
	"ic",
	"perhap",
	"throwing",
	"fev",
	"fiv",
	"canad",
	"mal",
	"libr",
	"southern",
	"altern",
	"per",
	"homeless",
	"tiny",
	"crap",
	"sibl",
	"resid",
	"cop",
	"street",
	"origin",
	"washington",
	"facebook",
	"georg",
	"bunch",
	"text",
	"hang",
	"pref",
	"grandfath",
	"scarc",
	"heck",
	"level",
	"map",
	"fing",
	"frust",
	"joy",
	"appl",
	"novemb",
	"ny",
	"london",
	"sat",
	"asleep",
	"closest",
	"print",
	"episod",
	"va",
	"bail",
	"occas",
	"dear",
	"huh",
	"pathet",
	"americ",
	"nowh",
	"anim",
	"tid",
	"emot",
	"cafeter",
	"convint",
	"atlant",
	"electron",
	"digit",
	"pag",
	"mod",
	"angel",
	"usernam",
	"homework",
	"u",
	"penny",
	"needless",
	"drunk",
	"shelt",
	"econom",
	"smok",
	"toronto",
	"dc",
	"wak",
	"ed",
	"yup",
	"marathon",
	"uh",
	"ta",
	"challeng",
	"assign",
	"fant",
	"engl",
	"bf",
	"bc",
	"essay",
	"ga",
	"bedroom",
	"cross",
	"wee"
}

def request_simpleWords(request):

	lancaster_stemmer = LancasterStemmer()
	request_words = nltk.tokenize.word_tokenize(request)
	
	request_words = [w.lower() for w in request_words]
	request_words = [w for w in request_words if w.isalpha() ]
	request_words = [lancaster_stemmer.stem(w) for w in request_words]
	request_words = set(request_words)
	
	features = {}
	for word in word_features:
		features['contains({})'.format(word)] = (word in request_words)
	return features
	
	
with io.open('train.json', 'r', encoding='utf-8') as f:
    training_data = json.load(f)
	
featuresets = []
for d in training_data:
	featuresets.append(( request_simpleWords(d["request_text"]) ,  d["requester_received_pizza"] ))
	
#train_set, test_set = featuresets[1000:], featuresets[:1000]
#classifier = nltk.classify.SklearnClassifier(BernoulliNB()).train(train_set)
classifier = nltk.classify.NaiveBayesClassifier.train(featuresets)
#print(nltk.classify.accuracy(classifier, test_set))
#print classifier.show_most_informative_features(20)


with open('test.json', 'r') as f:
    test_data = json.load(f)

f = open('testpredict-simpleWordsModel.csv', 'w')
f.write("request_id,requester_received_pizza\n");

for d in test_data:
	dprob = classifier.prob_classify(request_simpleWords(d["request_text_edit_aware"]))
	f.write("%s,%f\n" % (d["request_id"], dprob.prob(True)))

f.close()
	
	
	
